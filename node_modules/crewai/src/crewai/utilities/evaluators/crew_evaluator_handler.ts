import { ChatOpenAI } from 'langchain/chat_models/openai';
import { z } from 'zod';
import { Table } from 'console-table-printer';

import { Agent } from '../../agent';
import { Task } from '../../task';
import { TaskOutput } from '../../tasks/task_output';
import { Telemetry } from '../../telemetry';

const TaskEvaluationPydanticOutput = z.object({
  quality: z.number().describe("A score from 1 to 10 evaluating on completion, quality, and overall performance from the task_description and task_expected_output to the actual Task Output.")
});

class CrewEvaluator {
  constructor(crew, openaiModelName) {
    this.crew = crew;
    this.openaiModelName = openaiModelName;
    this._telemetry = new Telemetry();
    this.tasksScores = {};
    this.runExecutionTimes = {};
    this.iteration = 0;
    this._setupForEvaluating();
  }

  _setupForEvaluating() {
    for (const task of this.crew.tasks) {
      task.callback = this.evaluate.bind(this);
    }
  }

  _evaluatorAgent() {
    return new Agent({
      role: "Task Execution Evaluator",
      goal: "Your goal is to evaluate the performance of the agents in the crew based on the tasks they have performed using score from 1 to 10 evaluating on completion, quality, and overall performance.",
      backstory: "Evaluator agent for crew evaluation with precise capabilities to evaluate the performance of the agents in the crew based on the tasks they have performed",
      verbose: false,
      llm: new ChatOpenAI({ modelName: this.openaiModelName })
    });
  }

  _evaluationTask(evaluatorAgent, taskToEvaluate, taskOutput) {
    return new Task({
      description: `Based on the task description and the expected output, compare and evaluate the performance of the agents in the crew based on the Task Output they have performed using score from 1 to 10 evaluating on completion, quality, and overall performance.
        task_description: ${taskToEvaluate.description}
        task_expected_output: ${taskToEvaluate.expected_output}
        agent: ${taskToEvaluate.agent ? taskToEvaluate.agent.role : null}
        agent_goal: ${taskToEvaluate.agent ? taskToEvaluate.agent.goal : null}
        Task Output: ${taskOutput}`,
      expected_output: "Evaluation Score from 1 to 10 based on the performance of the agents on the tasks",
      agent: evaluatorAgent,
      output_pydantic: TaskEvaluationPydanticOutput
    });
  }

  setIteration(iteration) {
    this.iteration = iteration;
  }

  printCrewEvaluationResult() {
    const taskAverages = Object.values(this.tasksScores).reduce((acc, scores) => {
      scores.forEach((score, index) => {
        if (!acc[index]) acc[index] = [];
        acc[index].push(score);
      });
      return acc;
    }, []).map(scores => scores.reduce((a, b) => a + b, 0) / scores.length);

    const crewAverage = taskAverages.reduce((a, b) => a + b, 0) / taskAverages.length;

    const table = new Table({
      title: "Tasks Scores \n (1-10 Higher is better)",
      columns: [
        { name: "Tasks/Crew", alignment: "left" },
        ...Object.keys(this.tasksScores).map(run => ({ name: `Run ${parseInt(run) + 1}`, alignment: "right" })),
        { name: "Avg. Total", alignment: "right" }
      ]
    });

    taskAverages.forEach((avgScore, index) => {
      const taskScores = Object.values(this.tasksScores).map(scores => scores[index].toFixed(1));
      table.addRow({
        "Tasks/Crew": `Task ${index + 1}`,
        ...Object.fromEntries(taskScores.map((score, i) => [`Run ${i + 1}`, score])),
        "Avg. Total": avgScore.toFixed(1)
      });
    });

    const crewScores = Object.values(this.tasksScores).map(scores => 
      (scores.reduce((a, b) => a + b, 0) / scores.length).toFixed(1)
    );
    table.addRow({
      "Tasks/Crew": "Crew",
      ...Object.fromEntries(crewScores.map((score, i) => [`Run ${i + 1}`, score])),
      "Avg. Total": crewAverage.toFixed(1)
    });

    const runExecTimes = Object.values(this.runExecutionTimes).map(times => 
      Math.round(times.reduce((a, b) => a + b, 0))
    );
    const executionTimeAvg = Math.round(runExecTimes.reduce((a, b) => a + b, 0) / runExecTimes.length);
    table.addRow({
      "Tasks/Crew": "Execution Time (s)",
      ...Object.fromEntries(runExecTimes.map((time, i) => [`Run ${i + 1}`, time])),
      "Avg. Total": executionTimeAvg
    });

    table.printTable();
  }

  async evaluate(taskOutput) {
    const currentTask = this.crew.tasks.find(task => task.description === taskOutput.description);

    if (!currentTask || !taskOutput) {
      throw new Error("Task to evaluate and task output are required for evaluation");
    }

    const evaluatorAgent = this._evaluatorAgent();
    const evaluationTask = this._evaluationTask(evaluatorAgent, currentTask, taskOutput.raw);

    const evaluationResult = await evaluationTask.execute();

    if (TaskEvaluationPydanticOutput.safeParse(evaluationResult.pydantic).success) {
      this._testResultSpan = this._telemetry.individualTestResultSpan(
        this.crew,
        evaluationResult.pydantic.quality,
        currentTask._execution_time,
        this.openaiModelName
      );
      if (!this.tasksScores[this.iteration]) this.tasksScores[this.iteration] = [];
      this.tasksScores[this.iteration].push(evaluationResult.pydantic.quality);
      if (!this.runExecutionTimes[this.iteration]) this.runExecutionTimes[this.iteration] = [];
      this.runExecutionTimes[this.iteration].push(currentTask._execution_time);
    } else {
      throw new Error("Evaluation result is not in the expected format");
    }
  }
}

export { CrewEvaluator };