import { encoding_for_model } from '@dqbd/tiktoken';

class TokenCounterCallback {
  constructor() {
    this.totalTokens = 0;
    this.promptTokens = 0;
    this.completionTokens = 0;
  }

  onLLMStart(serialized, prompts, invocationParams) {
    const model = invocationParams.model || 'gpt-3.5-turbo';
    const encoding = encoding_for_model(model);
    this.promptTokens += prompts.reduce((sum, prompt) => sum + encoding.encode(prompt).length, 0);
  }

  onLLMEnd(response) {
    const model = response.llmOutput?.model_name || 'gpt-3.5-turbo';
    const encoding = encoding_for_model(model);
    this.completionTokens += encoding.encode(response.generations[0][0].text).length;
    this.totalTokens = this.promptTokens + this.completionTokens;
  }

  getTotalTokens() {
    return this.totalTokens;
  }

  getPromptTokens() {
    return this.promptTokens;
  }

  getCompletionTokens() {
    return this.completionTokens;
  }
}

export { TokenCounterCallback };